# -*- coding: utf-8 -*-
"""PneumoniaMNIST_Samriddhi_Sen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19QPJ8tQ2nwstOm1YbH7W5GWn0mxqUtNt
"""

# Pneumonia Detection from Chest X-Rays using InceptionV3

!pip install medmnist torch torchvision matplotlib seaborn pandas numpy -q

import medmnist
from medmnist import INFO, Evaluator
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
import torchvision.models as models
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import time
import copy
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

data_flag = 'pneumoniamnist'
download = True

info = INFO[data_flag]
task = info['task']
n_channels = info['n_channels']
n_classes = len(info['label'])

DataClass = getattr(medmnist, info['python_class'])

# Load the data
train_dataset = DataClass(split='train', download=download)
test_dataset = DataClass(split='test', download=download)

# Print dataset information
print("Train dataset size:", len(train_dataset))
print("Test dataset size:", len(test_dataset))
print("Number of channels:", n_channels)
print("Number of classes:", n_classes)
print("Task:", task)

# Data preprocessing and augmentation
data_transform = transforms.Compose([
    transforms.Resize((299, 299)), # Resize for InceptionV3
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.Grayscale(num_output_channels=3), # Convert to 3 channels
    transforms.ToTensor(),
    transforms.Normalize(mean=[.5,.5,.5], std=[.5,.5,.5]) # Adjust mean and std for 3 channels
])

# No augmentation for validation and test sets, only resizing and normalization
test_transform = transforms.Compose([
    transforms.Resize((299, 299)),
    transforms.Grayscale(num_output_channels=3), # Convert to 3 channels
    transforms.ToTensor(),
    transforms.Normalize(mean=[.5,.5,.5], std=[.5,.5,.5]) # Adjust mean and std for 3 channels
])

train_dataset.transform = data_transform
test_dataset.transform = test_transform

# Create data loaders
batch_size = 128
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

import torch.nn as nn
import torchvision.models as models

# Load the pre-trained InceptionV3 model
model = models.inception_v3(pretrained=True)

# Freeze all layers except the final classification layer
for name, param in model.named_parameters():
    if "fc" in name: # Unfreeze the fully connected layer
        param.requires_grad = True
    else:
        param.requires_grad = False

# Modify the output layer for binary classification (2 classes)
# InceptionV3 has an auxiliary output, so we need to modify both the main and auxiliary heads
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, n_classes)

# Modify the auxiliary output layer as well
if model.aux_logits:
    num_aux_ftrs = model.AuxLogits.fc.in_features
    model.AuxLogits.fc = nn.Linear(num_aux_ftrs, n_classes)


# Define the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print("Model defined and moved to:", device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10  # You can adjust this
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        # InceptionV3 returns main output and auxiliary output during training
        outputs, aux_outputs = model(inputs)
        loss1 = criterion(outputs, labels.squeeze())
        loss2 = criterion(aux_outputs, labels.squeeze())
        loss = loss1 + 0.4 * loss2 # Combine losses

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * inputs.size(0)

    epoch_loss = running_loss / len(train_dataset)
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}")

print("Training finished.")

# Model evaluation
model.eval()
running_corrects = 0
all_labels = []
all_preds = []

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)

        # InceptionV3 returns a tuple of (output, auxiliary_output) during evaluation
        # We only need the main output for evaluation
        if isinstance(outputs, tuple):
            outputs = outputs[0]

        _, preds = torch.max(outputs, 1)

        all_labels.extend(labels.cpu().numpy())
        all_preds.extend(preds.cpu().numpy())

# Calculate evaluation metrics
accuracy = accuracy_score(all_labels, all_preds)
precision = precision_score(all_labels, all_preds)
recall = recall_score(all_labels, all_preds)
f1 = f1_score(all_labels, all_preds)
conf_matrix = confusion_matrix(all_labels, all_preds)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

# Analyze class distribution in the training dataset
train_labels = [label for _, label in train_dataset]
class_counts = pd.Series(train_labels).value_counts().sort_index()

print("Class distribution in the training dataset:")
print(class_counts)

# Calculate and print the percentages
total_samples = len(train_labels)
class_percentages = (class_counts / total_samples) * 100
print("\nClass percentages in the training dataset:")
print(class_percentages)

!pip freeze > requirements.txt

import matplotlib.pyplot as plt
import numpy as np

# Get predictions and true labels for the test set
all_preds = []
all_labels = []
all_inputs = []

model.eval()
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        if isinstance(outputs, tuple):
            outputs = outputs[0] # Get main output for InceptionV3

        _, preds = torch.max(outputs, 1)

        all_inputs.extend(inputs.cpu())
        all_labels.extend(labels.cpu().numpy())
        all_preds.extend(preds.cpu().numpy())

# Identify misclassified indices
misclassified_indices = [i for i, (label, pred) in enumerate(zip(all_labels, all_preds)) if label != pred]

print(f"Number of misclassified cases: {len(misclassified_indices)}")

# Display a few misclassified examples
num_display = 5 # Number of misclassified examples to display
if len(misclassified_indices) > 0:
    plt.figure(figsize=(15, 5))
    for i in range(min(num_display, len(misclassified_indices))):
        index = misclassified_indices[i]
        input_img = all_inputs[index]
        true_label = all_labels[index]
        predicted_label = all_preds[index]

        # Denormalize the image for display
        # Assuming normalization with mean=[.5,.5,.5] and std=[.5,.5,.5]
        img_display = input_img * 0.5 + 0.5
        img_display = np.clip(img_display.permute(1, 2, 0), 0, 1) # Rearrange dimensions for matplotlib

        plt.subplot(1, num_display, i + 1)
        plt.imshow(img_display)
        plt.title(f"True: {true_label}\nPred: {predicted_label}")
        plt.axis('off')
    plt.show()
else:
    print("No misclassified cases found in the test set.")